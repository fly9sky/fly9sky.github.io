---
layout: post
title:  数学学习内容总结
description: 数学，算法，数据结构等学习归纳总结
date: 2022-10-01 09:01:01
---

- [微积分](#微积分)
- [矩阵](#矩阵)
  - [一、矩阵加法](#一矩阵加法)
- [数据结构](#数据结构)
- [算法](#算法)
- [AI人工智能深度学习](#ai人工智能深度学习)
- [神经网络](#神经网络)
  - [什么是神经网络](#什么是神经网络)
  - [激活函数](#激活函数)
  - [卷积神经网络](#卷积神经网络)
  - [RNN循环神经网络](#rnn循环神经网络)
  - [LSTM （Long Short-Term Memory）](#lstm-long-short-term-memory)
- [备注](#备注)

## 微积分

## 矩阵

### 一、矩阵加法 

> 两个矩阵相加或相减，需要满足两个矩阵的列数和行数一致。

> 加法交换律：A + B = B + A
a + b = [ a + a 1 b + a 2 c + b 1 d + b 2 ] (1) a+b= \left[
a+a1c+b1b+a2d+b2
�
+
�
1
�
+
�
2
�
+
�
1
�
+
�
2
\right] \tag{1}
a+b=[ 
a+a1
c+b1
​
  
b+a2
d+b2
​
 ](1)

二、矩阵减法
a − b = [ a − a 1 b − a 2 c − b 1 d − b 2 ] (1) a-b= \left[
a−a1c−b1b−a2d−b2
�
−
�
1
�
−
�
2
�
−
�
1
�
−
�
2
\right] \tag{1}
a−b=[ 
a−a1
c−b1
​
  
b−a2
d−b2
​
 ](1)

三、矩阵乘法
两个矩阵A和B相乘，需要满足A的列数等于B的行数。
a矩阵的行元素乘以每一列然后相加作为新矩阵的行元素
a ∗ b = [ a ∗ a 1 + b ∗ b 1 a ∗ a 2 + b ∗ b 2 c ∗ a 1 + d ∗ b 1 c ∗ a 2 + d ∗ b 2 ] (1) a*b= \left[
a∗a1+b∗b1c∗a1+d∗b1a∗a2+b∗b2c∗a2+d∗b2
�
∗
�
1
+
�
∗
�
1
�
∗
�
2
+
�
∗
�
2
�
∗
�
1
+
�
∗
�
1
�
∗
�
2
+
�
∗
�
2
\right] \tag{1}
a∗b=[ 
a∗a1+b∗b1
c∗a1+d∗b1
​
  
a∗a2+b∗b2
c∗a2+d∗b2
​
 ](1)

矩阵乘法不满足交换律，但是满足分配率和结合律,也就是说AB不等于BA
(AB)C=A(BC)
(A+B)C=AC+BC
C(A+B)=CA+CB

g*(AB)=(gA)B=A(gB)
g属于实数

四、矩阵转置
a矩阵转置之后(行变成列，列变成行)

a T = [ a c b d ] (1) aT= \left[
abcd
�
�
�
�
\right] \tag{1}
aT=[ 
a
b
​
  
c
d
​
 ](1)

转置性质：：


五、逆矩阵
矩阵A的逆矩阵记作A’， A A’=A’A= I，I是单位矩阵。
先介绍一下矩阵的单位阵，就是单位矩阵是一个n×n矩阵，从左到右的对角线上的元素是1，其余元素都为0。

c = [ 1 0 0 1 ] (1) c= \left[
1001
1
0
0
1
\right] \tag{1}
c=[ 
1
0
​
  
0
1
​
 ](1)

d = [ 1 0 0 0 1 0 0 0 1 ] (2) d= \left[
100010001
1
0
0
0
1
0
0
0
1
\right] \tag{2}
d= 
⎣
⎡
​
  
1
0
0
​
  
0
1
0
​
  
0
0
1
​
  
⎦
⎤
​
 (2)

性质：ac=ca=a


六、对称矩阵
如果一个矩阵转置后等于原矩阵，那么这个矩阵称为对称矩阵。由定义可知，对称矩阵一定是方阵。一个矩阵转置和这个矩阵的乘积就是一个对称矩阵：


七、矩阵性质总结
性质1：单位矩阵的行列式为 1 ，与之对应的是单位立方体的体积是 1。
性质 2：当两行进行交换的时候行列式改变符号。
性质 3：行列式是单独每一行的线性函数（其它行不变）。
性质4：矩阵中有俩行一样，矩阵的行列式为0。
性质 5：用矩阵的一行减去另一行的倍数，行列式不变。
性质 6：当矩阵的某一行全为零的时候，行列式为零。
性质 7：如果矩阵是三角形的，那么行列式等于对角线上元素的乘积。
d = [ 1 0 0 0 1 0 0 0 1 ] (2) d= \left[
100010001
1
0
0
0
1
0
0
0
1
\right] \tag{2}
d= 
⎣
⎡
​
  
1
0
0
​
  
0
1
0
​
  
0
0
1
​
  
⎦
⎤
​
 (2)

性质 8：如果矩阵是可逆的那么矩阵的行列式不等于0，反之行列式为0

————————————————
版权声明：本文为CSDN博主「月疯」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/chehec2010/article/details/116242992

## 数据结构

1.集合：数据结构中的元素之间除了“同属一个集合” 的相互关系外，别无其他关系； [1] 
2.线性结构：数据结构中的元素存在一对一的相互关系； [1] 
3.树形结构：数据结构中的元素存在一对多的相互关系； [1] 
4.图形结构：数据结构中的元素存在多对多的相互关系。 [1] 

在计算机科学的发展过程中，数据结构也随之发展。程序设计中常用的数据结构包括如下几个。 [5] 
数组(Array)
数组是一种聚合数据类型，它是将具有相同类型的若干变量有序地组织在一起的集合。数组可以说是最基本的数据结构，在各种编程语言中都有对应。一个数组可以分解为多个数组元素，按照数据元素的类型，数组可以分为整型数组、字符型数组、浮点型数组、指针数组和结构数组等。数组还可以有一维、二维以及多维等表现形式。 [5] 
栈( Stack)
栈是一种特殊的线性表，它只能在一个表的一个固定端进行数据结点的插入和删除操作。栈按照先进后出或后进先出的原则来存储数据，也就是说，先插入的数据将被压入栈底，最后插入的数据在栈顶，读出数据时，从栈顶开始逐个读出。栈在汇编语言程序中，经常用于重要数据的现场保护。栈中没有数据时，称为空栈。 [5] 
队列(Queue)
队列和栈类似，也是一种特殊的线性表。和栈不同的是，队列只允许在表的一端进行插入操作，而在另一端进行删除操作。一般来说，进行插入操作的一端称为队尾，进行删除操作的一端称为队头。队列中没有元素时，称为空队列。 [5] 
链表( Linked List)
链表是一种数据元素按照链式存储结构进行存储的数据结构，这种存储结构具有在物理上存在非连续的特点。链表由一系列数据结点构成，每个数据结点包括数据域和指针域两部分。其中，指针域保存了数据结构中下一个元素存放的地址。链表结构中数据元素的逻辑顺序是通过链表中的指针链接次序来实现的。 [5] 
树( Tree)
树是典型的非线性结构，它是包括，2个结点的有穷集合K。在树结构中，有且仅有一个根结点，该结点没有前驱结点。在树结构中的其他结点都有且仅有一个前驱结点，而且可以有两个后继结点，m≥0。 [5] 
图(Graph)
图是另一种非线性数据结构。在图结构中，数据结点一般称为顶点，而边是顶点的有序偶对。如果两个顶点之间存在一条边，那么就表示这两个顶点具有相邻关系。 [5] 
堆(Heap)
堆是一种特殊的树形数据结构，一般讨论的堆都是二叉堆。堆的特点是根结点的值是所有结点中最小的或者最大的，并且根结点的两个子树也是一个堆结构。 [5] 
散列表(Hash)
散列表源自于散列函数(Hash function)，其思想是如果在结构中存在关键字和T相等的记录，那么必定在F(T)的存储位置可以找到该记录，这样就可以不用进行比较操作而直接取得所查记录。 [5] 

## 算法

数据结构研究的内容：就是如何按一定的逻辑结构，把数据组织起来，并选择适当的存储表示方法把逻辑结构组织好的数据存储到计算机的存储器里。算法研究的目的是为了更有效的处理数据，提高数据运算效率。数据的运算是定义在数据的逻辑结构上，但运算的具体实现要在存储结构上进行。一般有以下几种常用运算： [3] 

(1)检索。检索就是在数据结构里查找满足一定条件的节点。一般是给定一个某字段的值，找具有该字段值的节点。 [3] 

(2)插入。往数据结构中增加新的节点。 [3] 

(3)删除。把指定的结点从数据结构中去掉。 [3] 

(4)更新。改变指定节点的一个或多个字段的值。 [3] 

(5)排序。把节点按某种指定的顺序重新排列。例如递增或递减。 [3] 

章 导论
1.1 什么是算法
1.2 算法规范
1.2.1 引言
1.2.2 递归算法
1.3 性能分析
1.3.1 空间复杂度
1.3.2 时间复杂度
1.3.3 渐近符号 （O、 佟龋?
1.3.4 实际复杂度
1.3.5 性能度量
1.4 随机算法
1.4.1 概率论基础
1.4.2 随机算法： 非形式化的描述
1.4.3 识别重复元素
1.4.4 素数测试
1.4.5 优点与缺点
1.5 参考文献和读物
第2章 基本数据结构
2.1 栈和队列
2.2 树
2.2.1 术语
2.2.2 二叉树
2.3 字典
2.3.1 二叉查找树
2.3.2 代价分摊
2.4 优先队列
2.4.1 堆
2.4.2 堆排序
2.5 集合和不相交集合的并
2.5.1 概述
2.5.2 并和查找操作
2.6 图
2.6.1 概述
2.6.2 定义
2.6.3 图的表示方法
2.7 参考文献和读物
第3章 分治算法
3.1 一般方法
3.2 二叉查找
3.3 查找值和值
3.4 归并排序
3.5 快速排序
3.5.1 性能度量
3.5.2 随机排序算法
3.6 选择
3.6.1 最坏情况下的算法
3.6.2 Select2的实现
3.7 Strassen矩阵乘法
3.8 凸包
3.8.1 几种原始几何方法
3.8.2 QuickHull算法
3.8.3 Graham扫描算法
3.8.4 一个O(nlogn)的分治算法
3.9 参考文献和读物
3.10 附加习题
第4章 贪心算法
4.1 一般方法
4.2 背包问题
4.3 树节点分割
4.4 带有期限的作业顺序问题
4.5 代价生成树
4.5.1 Prim算法
4.5.2 Kruskal算法
4.5.3 一个随机算法（*）
4.6 磁带的存储
4.7 归并模式
4.8 单源最短路径
4.9 参考文献和读物
4.10 附加习题
第5章 动态规划
5.1 一般方法
5.2 多部图
5.3 每一对顶点之间的最短路径
5.4 单源最短路径： 普通权值
5.5 二叉查找树
5.6 串编辑
5.7 0/1背包
5.8 可靠性设计
5.9 旅行商问题
5.10 流水作业调度
5.11 参考文献和读物
5.12 附加习题
第6章 基本的查找和遍历技术
6.1 二叉树算法
6.2 图算法
6.2.1 广度优先搜索和遍历
6.2.2 深度优先搜索和遍历
6.3 连通分支和生成树
6.4 双连通分支和DFS算法
6.5 参考文献和读物
第7章 回溯
7.1 一般方法
7.2 8皇后问题
7.3 子集求和问题
7.4 图的着色
7.5 哈密顿回路
7.6 背包问题
7.7 参考文献和读物
7.8 附加习题
第8章 分支限界法
8.1 一般方法
8.1.1 代价查找
8.1.2 15拼板： 一个例子
8.1.3 LC查找的控制抽象
8.1.4 限界
8.1.5 FIFO分支限界法
8.1.6 LC分支限界法
8.2 0/1背包问题
8.2.1 LC分支限界法求解
8.2.2 FIFO分支限界法求解
8.3 旅行商问题
8.4 效率因素
8.5 参考文献和读物
第9章 代数问题
9.1 一般方法
9.2 计算和插值
9.3 快速傅里叶变换
9.3.1 FFT的原地版本
9.3.2 一些保留问题
9.4 模运算
9.5 更快的计算和插值
9.6 参考文献和读物
0章 下界理论
10.1 比较树
10.1.1 有序查找
10.1.2 排序
10.1.3 选择
10.2 喻示和对立论
10.2.1 归并
10.2.2 和次大
10.2.3 状态空间方法
10.2.4 选择
10.3 通过规约求下界
10.3.1 寻找凸包
10.3.2 不相交集合问题
10.3.3 即时中值查找
10.3.4 三角矩阵相乘
10.3.5 下三角矩阵求逆
10.3.6 计算传递闭包
10.4 解决代数问题的技术
10.5 参考文献和读物
1章 难与完全问题
11.1 基本概念
11.1.1 非确定性算法
11.1.2 难和完全类
11.2 Cook定理（*）
11.3 难的图问题
11.3.1 团判定问题
11.3.2 节点覆盖判定问题
11.3.3 色数判定问题
11.3.4 有向哈密顿回路（*）
11.3.5 旅行商判定问题
11.3.6 与/或图判定问题
11.4 难的调度问题
11.4.1 调度相同的处理器
11.4.2 流水作业调度
11.4.3 作业安排调度
11.5 难的代码生成问题
11.5.1 带有公共子表达式的代码生成
11.5.2 实现并行赋值指令
11.6 一些简化的难问题
11.7 参考文献和读物
11.8 附加习题
2章 近似算法
12.1 概述
12.2 近似
12.2.1 平面图着色
12.2.2 最多程序存储问题
12.2.3 难的近似
12.3 褰?
12.3.1 独立任务的调度
12.3.2 装箱问题
12.3.3 难的褰莆侍?
12.4 多项式时间近似方案
12.4.1 独立任务的调度
12.4.2 0/1背包
12.5 完全多项式时间近似方案
12.5.1 舍入法
12.5.2 区间划分法
12.5.3 分割法
12.6 概率上的好算法
12.7 参考文献和读物
12.8 附加习题
3章 PRAM算法
13.1 概述
13.2 计算模型
13.3 基本技术和算法
13.3.1 前缀计算
13.3.2 表排序
13.4 选择
13.4.1 使用n?2个处理器选择值
13.4.2 使用n个处理器选择值
13.4.3 在整数中选择值
13.4.4 使用n?2个处理器的一般选择问题
13.4.5 一个工作的随机算法
13.5 归并
13.5.1 对数时间算法
13.5.2 奇偶归并
13.5.3 工作的算法
13.5.4 O(log logm)时间算法
13.6 排序
13.6.1 奇偶归并排序
13.6.2 随机选择算法
13.6.3 Preparata算法
13.6.4 Reischuk随机算法（*）
13.7 图问题
13.7.1 计算传递闭包的另一种算法
13.7.2 每一对顶点之间的最短路径
13.8 计算凸包
13.9 下界
13.9.1 平均情况下排序的下界
13.9.2 寻找值
13.10 参考文献和读物
13.11 附加习题
4章 网格算法
14.1 计算模型
14.2 分组路由
14.2.1 线性阵列中的分组路由
14.2.2 网格上PPR的贪心算法
14.2.3 具有小队列的随机算法
14.3 基本算法
14.3.1 广播
14.3.2 前缀计算
14.3.3 数据集中
14.3.4 稀疏枚举排序
14.4 选择
14.4.1 n=p时的随机算法
14.4.2 n＞p时的随机选择
14.4.3 n＞p时的确定性算法
14.5 归并
14.5.1 在线性阵列上的顺序号归并
14.5.2 线性阵列上的奇偶归并
14.5.3 在网格上的奇偶归并
14.6 排序
14.6.1 在线性阵列上的排序
14.6.2 在网格上的排序
14.7 图问题
14.7.1 传递闭包的n×n网格算法
14.7.2 每一对顶点之间的最短路径
14.8 计算凸包
14.9 参考文献和读物
14.10 附加习题
5章 超立方体算法
15.1 计算模型
15.1.1 超立方体
15.1.2 蝶形网络
15.1.3 其他网络的嵌入
15.2 PPR路由
15.2.1 贪心算法
15.2.2 随机算法
15.3 基本算法
15.3.1 广播
15.3.2 前缀计算
15.3.3 数据集中
15.3.4 稀疏枚举排序
15.4 选择
15.4.1 n=p时的随机算法
15.4.2 n＞p时的随机选择
15.4.3 n＞p时的确定性算法
15.5 归并
15.5.1 奇偶归并
15.5.2 双调谐归并
15.6 排序
15.6.1 奇偶归并排序
15.6.2 双调谐排序
15.7 图问题
15.8 计算凸包
15.9 参考文献和读物
15.10 附加习题
索引
Introduction to Algorithms, Third Edition
出版者的话
译者序
前言
部分　基础知识
第1章　算法在计算中的作用
　1.1　算法
　1.2　作为一种技术的算法
　思考题
　本章注记
第2章　算法基础
　2.1　插入排序
　2.2　分析算法
　2.3　设计算法
　　2.3.1　分治法
　　2.3.2　分析分治算法
　思考题
　本章注记
第3章　函数的增长
　3.1　渐近记号
　3.2　标准记号与常用函数
　思考题
　本章注记
第4章　分治策略
　4.1　子数组问题
　4.2　矩阵乘法的Strassen算法
　4.3　用代入法求解递归式
　4.4　用递归树方法求解递归式
　4.5　用主方法求解递归式
　4.6　证明主定理
　　4.6.1　对b的幂证明主定理
　　4.6.2　向下取整和向上取整
　思考题
　本章注记
第5章　概率分析和随机算法
　5.1　雇用问题
　5.2　指示器随机变量
　5.3　随机算法
　?5.4　概率分析和指示器随机变量的进一步使用
　　5.4.1　生日悖论
　　5.4.2　球与箱子
　　5.4.3　特征序列
　　5.4.4　在线雇用问题
　思考题
　本章注记
第二部分　排序和顺序统计量
第6章　堆排序
　6.1　堆
　6.2　维护堆的性质
　6.3　建堆
　6.4　堆排序算法
　6.5　优先队列
　思考题
　本章注记
第7章　快速排序
　7.1　快速排序的描述
　7.2　快速排序的性能
　7.3　快速排序的随机化版本
　7.4　快速排序分析
　　7.4.1　坏情况分析
　　7.4.2　期望运行时间
　思考题
　本章注记
第8章　线性时间排序
　8.1　排序算法的下界
　8.2　计数排序
　8.3　基数排序
　8.4　桶排序
　思考题
　本章注记
第9章　中位数和顺序统计量
　9.1　小值和值
　9.2　期望为线性时间的选择算法
　9.3　坏情况为线性时间的选择算法
　思考题
　本章注记
第三部分　数据结构
第10章　基本数据结构
　10.1　栈和队列
　10.2　链表
　10.3　指针和对象的实现
　10.4　有根树的表示
　思考题
　本章注记
第11章　散列表
　11.1　直接寻址表
　11.2　散列表
　11.3　散列函数
　　11.3.1　除法散列法
　　11.3.2　乘法散列法
　　11.3.3　全域散列法
　11.4　开放寻址法
　11.5　完全散列
　思考题
　本章注记
第12章　二叉搜索树
　12.1　什么是二叉搜索树
　12.2　查询二叉搜索树
　12.3　插入和删除
　12.4　随机构建二叉搜索树
　思考题
　本章注记
第13章　红黑树
　13.1　红黑树的性质
　13.2　旋转
　13.3　插入
　13.4　删除
　思考题
　本章注记
第14章　数据结构的扩张
　14.1　动态顺序统计
　14.2　如何扩张数据结构
　14.3　区间树
　思考题
　本章注记
第四部分　高级设计和分析技术
第15章　动态规划
　15.1　钢条切割
　15.2　矩阵链乘法
　15.3　动态规划原理
　15.4　长公共子序列
　15.5　二叉搜索树
　思考题
　本章注记
第16章　贪心算法
　16.1　活动选择问题
　16.2　贪心算法原理
　16.3　赫夫曼编码
　16.4　拟阵和贪心算法
　16.5　用拟阵求解任务调度问题
　思考题
　本章注记
第17章　摊还分析
　17.1　聚合分析
　17.2　核算法
　17.3　势能法
　17.4　动态表
　　17.4.1　表扩张
　　17.4.2　表扩张和收缩
　思考题
　本章注记
第五部分　高级数据结构
第18章　B树
　18.1　B树的定义
　18.2　B树上的基本操作
　18.3　从B树中删除关键字
　思考题
　本章注记
第19章　斐波那契堆
　19.1　斐波那契堆结构
　19.2　可合并堆操作
　19.3　关键字减值和删除一个结点
　19.4　度数的界
　思考题
　本章注记
第20章　van Emde Boas树
　20.1　基本方法
　20.2　递归结构
　　20.2.1　原型van Emde Boas结构
　　20.2.2　原型van Emde Boas结构上的操作
　20.3　van Emde Boas树及其操作
　　20.3.1　van Emde Boas树
　　20.3.2　van Emde Boas树的操作
　思考题
　本章注记
第21章　用于不相交集合的数据结构
　21.1　不相交集合的操作
　21.2　不相交集合的链表表示
　21.3　不相交集合森林
　*21.4　带路径压缩的按秩合并的分析
　思考题
　本章注记
第六部分　图算法
第22章　基本的图算法
　22.1　图的表示
　22.2　广度优先搜索
　22.3　深度优先搜索
　22.4　拓扑排序
　22.5　强连通分量
　思考题
　本章注记
第23章　小生成树
　23.1　小生成树的形成
　23.2　Kruskal算法和Prim算法
　思考题
　本章注记
第24章　单源短路径
　24.1　Bellman?Ford算法
　24.2　有向无环图中的单源短路径问题
　24.3　Dijkstra算法
　24.4　差分约束和短路径
　24.5　短路径性质的证明
　思考题
　本章注记
第25章　所有结点对的短路径问题
　25.1　短路径和矩阵乘法
　25.2　Floyd?Warshall算法
　25.3　用于稀疏图的Johnson算法
　思考题
　本章注记
第26章　流
　26.1　流网络
　26.2　Ford\Fulkerson方法
　26.3　二分匹配
　26.4　推送重贴标签算法
　26.5　前置重贴标签算法
　思考题
　本章注记
第七部分　算法问题选编
第27章　多线程算法
　27.1　动态多线程基础
　27.2　多线程矩阵乘法
　27.3　多线程归并排序
　思考题
　本章注记
第28章　矩阵运算
　28.1　求解线性方程组
　28.2　矩阵求逆
　28.3　对称正定矩阵和小二乘逼近
　思考题
　本章注记
第29章　线性规划
　29.1　标准型和松弛型
　29.2　将问题表达为线性规划
　29.3　单纯形算法
　29.4　对偶性
　29.5　初始基本可行解
　思考题
　本章注记
第30章　多项式与快速傅里叶变换
　30.1　多项式的表示
　30.2　DFT与FFT
　30.3　高效FFT实现
　思考题
　本章注记
第31章　数论算法
　31.1　基础数论概念
　31.2　公约数
　31.3　模运算
　31.4　求解模线性方程
　31.5　中国余数定理
　31.6　元素的幂
　31.7　RSA公钥加密系统
　31.8　素数的测试
　31.9　整数的因子分解
　思考题
　本章注记
第32章　字符串匹配
　32.1　朴素字符串匹配算法
　32.2　Rabin\Karp算法
　32.3　利用有限自动机进行字符串匹配
　32.4　Knuth?Morris?Pratt算法
　思考题
　本章注记
第33章　计算几何学
　33.1　线段的性质
　33.2　确定任意一对线段是否相交
　33.3　寻找凸包
　33.4　寻找近点对
　思考题
　本章注记
第34章　NP完全性
　34.1　多项式时间
　34.2　多项式时间的验证
　34.3　NP完全性与可归约性
　34.4　NP完全性的证明
　34.5　NP完全问题
　　34.5.1　团问题
　　34.5.2　顶点覆盖问题
　　34.5.3　哈密顿回路问题
　　34.5.4　旅行商问题
　　34.5.5　子集和问题
　思考题
　本章注记
第35章　近似算法
　35.1　顶点覆盖问题
　35.2　旅行商问题
　35.2.1　满足三角不等式的旅行商问题
　35.2.2　一般旅行商问题
　35.3　集合覆盖问题
　35.4　随机化和线性规划
　35.5　子集和问题
　思考题
　本章注记
第八部分　附录：数学基础知识
附录A　求和
　A.1　求和公式及其性质
　A.2　确定求和时间的界
　思考题
　附录注记
附录B　集合等离散数学内容
　B.1　集合
　B.2　关系
　B.3　函数
　B.4　图
　B.5　树
　　B.5.1　自由树
　　B.5.2　有根树和有序树
　　B.5.3　二叉树和位置树
　思考题
　附录注记
附录C　计数与概率
　C.1　计数
　C.2　概率
C.3　离散随机变量
　C.4　几何分布与二项分布
　*C.5　二项分布的尾部
　思考题
　附录注记
附录D　矩阵
　D.1　矩阵与矩阵运算
　D.2　矩阵基本性质
　思考题
　附录注记
参考文献
索引


## AI人工智能深度学习

机器学习基础知识篇

第1章 人工智能概述    002
1.1 人工智能的定义       002
1.2 人工智能发展简史    003
1.2.1 史前文明，曙光初现(1956年前)    004
1.2.2 初出茅庐，一战成名(1956—1974年)    008
1.2.3 寒风凛冽，首次入冬(1974—1980年)    010
1.2.4 卷土重来，威震八方(1980—1987年)    010
1.2.5 失望弥漫，再度入冬(1987—1993年)    012
1.2.6 重出江湖，渐入佳境(1993年至今)    013
1.3 人工智能经典流派    016
1.3.1 符号主义       018
1.3.2 连接主义       019
1.3.3 行为主义       023
1.3.4 贝叶斯派       026
1.4 人工智能与机器学习    027
1.5 如何选择机器学习算法    029
1.5.1 没有免费的午餐理论    030
1.5.2 Scikit Learn小抄    031
1.5.3 Microsoft Azure小抄    032
1.6 机器学习的典型应用场景    032
1.6.1 计算机图像领域    035
1.6.2 自然语言处理简述及其应用    036
1.6.3 制造业中的预测性维护    038
1.6.4 软件自动化开发和测试    042
1.7 本书的组织结构       043

第2章 机器学习中的数学基础    045
2.1 微分学          045
2.1.1 链式求导法则    045
2.1.2 对数函数求导    045
2.1.3 梯度和梯度下降算法    046
2.2 线性代数       047
2.2.1 向量          047
2.2.2 矩阵拼接       052
2.2.3 特征值和特征向量    057
2.2.4 仿射变换       059
2.3 概率论          060
2.3.1 概率分布       061
2.3.2 先验/后验概率    062
2.3.3 似然估计    063
2.3.4 贝叶斯法则    064
2.4 统计学          065
2.4.1 数据的标准化和归一化    065
2.4.2 标准差       066
2.4.3 方差和偏差    066
2.4.4 协方差和协方差矩阵    067
2.5 化理论          068
2.5.1 概述          068
2.5.2 函数等高线    070
2.5.3 拉格朗日乘子法    071
2.5.4 拉格朗日对偶性    074
2.5.5 KKT          079
2.6 其他             088
2.6.1 训练、验证和测试数据集    088
2.6.2 过拟合和欠拟合    090
2.6.3 奥卡姆的剃刀    092
2.6.4 信息熵       093
2.6.5 IOU          094
2.6.6 NMS          095
2.6.7 Huffman树    096

第3章 机器学习模型的度量指标    099
3.1 Precision、Recall和mAP    099
3.2 F1 Score       101
3.3 混淆矩阵       102
3.4 ROC       103
3.5 AUC       105
3.6 PRC       107
3.7 工业界使用的典型AI指标    108

经典机器学习篇

第4章 回归算法    112
4.1 回归分析       112
4.2 线性回归       112
4.2.1 线性回归的定义    112
4.2.2 线性回归的损失函数    113
4.2.3 线性回归范例    113
4.3 逻辑回归       115
4.3.1 逻辑回归—二分类    115
4.3.2 逻辑回归—多分类及Softmax    119

第5章 K-NN算法    122
5.1 K-NN概述          122
5.2 K-NN分类算法       123
5.3 K-NN回归算法       124
5.4 K-NN的优缺点       125
5.4.1 K-NN的优点    125
5.4.2 K-NN的缺点    126
5.5 K-NN工程范例       126

第6章 k-means       129
6.1 k-means概述       129
6.2 k-means核心算法    129
6.3 k-means算法的优缺点    131
6.3.1 k-means算法的优点    131
6.3.2 k-means算法的缺点    131
6.4 k-means工程范例    132

第7章 朴素贝叶斯    135
7.1 朴素贝叶斯分类算法    135
7.2 朴素贝叶斯的实际应用    137

第8章 决策树和随机森林    141
8.1 决策树          141
8.1.1 决策树的主要组成元素    141
8.1.2 决策树的经典算法    141
8.1.3 决策树的优缺点    145
8.1.4 决策树的过拟合和剪枝    145
8.2 随机森林       146

第9章 支持向量机    149
9.1 SVM可以做什么    149
9.2 SVM的数学表述    151
9.2.1 决策面的数学表述    151
9.2.2 分类间隔的数学表述    152
9.2.3 比较超平面的数学公式    153
9.2.4 决策面的数学表述    159
9.3 SVM相关的化理论    160
9.3.1 感知机学习算法    160
9.3.2 SVM化问题    173
9.4 硬间隔SVM          174
9.5 软间隔SVM          177
9.6 核函数技巧          182
9.7 多分类SVM          187
9.8 SVM实践       193

第10章 PCA降维    196
10.1 降维概述          196
10.2 PCA降维实现原理    197
10.2.1 PCA的直观理解    197
10.2.2 PCA的理论基础—方差理论    199
10.2.3 PCA的核心处理过程    199
10.3 PCA实例          200

第11章 集成学习    202
11.1 集成学习概述       202
11.2 集成学习架构       203
11.2.1 聚合法       203
11.2.2 提升法       204
11.2.3 堆叠法       205
11.3 典型的集成方法    206
11.3.1 平均法       206
11.3.2 投票法       207
11.3.3 学习法       208

深度学习进阶篇

第12章 深度神经网络    212
12.1 神经元          212
12.2 激活函数          214
12.2.1 Sigmoid    214
12.2.2 tanh       216
12.2.3 ReLU       217
12.2.4 Leaky ReLU    218
12.2.5 ReLU的其他变种    219
12.2.6 激活函数的选择    220
12.3 前向传播和后向传播算法    220
12.4 损失函数          224
12.4.1 分类场景    225
12.4.2 回归场景    228
12.4.3 其他任务类型的损失函数    230

第13章 卷积神经网络    232
13.1 CNN发展历史简述   232
13.2 CNN的核心组成元素   233
13.2.1 卷积层       233
13.2.2 池化层       235
13.2.3 全连接层    236
13.2.4 Softmax层    237
13.3 CNN经典框架      237
13.3.1 LeNet       237
13.3.2 AlexNet    238
13.3.3 VGG       240
13.3.4 GoogLeNet    242
13.3.5 ResNet       245
13.4 CNN的典型特性   249
13.4.1 CNN位移不变性    250
13.4.2 CNN尺度不变性    252
13.4.3 CNN旋转不变性    253
13.4.4 CNN视角不变性    255

第14章 RNN与LSTM    256
14.1 RNN         256
14.2 RNN的多种形态   257
14.3 RNN存在的不足   258
14.4 LSTM          259
14.5 LSTM核心框架    259
14.5.1 遗忘门       261
14.5.2 输入门       261
14.5.3 输出门       262
14.6 GRU         263

第15章 深度强化学习    265
15.1 强化学习和MDP    265
15.1.1 强化学习的基础概念    265
15.1.2 MDP       266
15.1.3 强化学习的核心三要素    267
15.2 MDP问题的解决方案分类    268
15.3 基于模型的动态规划算法    269
15.4 基于无模型的强化学习算法    272
15.4.1 蒙特·卡罗强化学习算法    272
15.4.2 时间差分算法    275
15.5 DQN          278
15.6 基于策略的强化学习算法    280
15.6.1 有限差分策略梯度    283
15.6.2 蒙特·卡罗策略梯度    283

第16章 MCTS       285
16.1 MCTS概述          285
16.2 MCTS算法核心处理过程    286
16.3 UCB和UCT       286
16.4 MCTS实例解析    288

机器学习应用实践及相关原理

第17章 数据集的建设    292
17.1 数据集建设的核心目标    292
17.2 数据采集和标注    294
17.2.1 数据从哪来    294
17.2.2 数据分布和多样性    296
17.2.3 如何扩大数据量    298
17.3 数据分析和处理    299
17.3.1 数据集分析的典型方法    299
17.3.2 标签类别合理性    301
17.3.3 数据清洗    303

第18章 CNN训练技巧    304
18.1 数据预处理       304
18.1.1 数据零中心化    304
18.1.2 数据标准化    306
18.1.3 尺寸调整    306
18.1.4 其他       307
18.2 数据增强          308
18.3 CNN核心组件择优   309
18.3.1 激活函数    309
18.3.2 超参数设定    309
18.4 参数初始化策略    310
18.4.1 全零初始化策略    310
18.4.2 随机初始化策略    311
18.4.3 采用预训练模型    319
18.5 模型过拟合解决方法    319
18.5.1 正则化       319
18.5.2 批标准化    320
18.6 模型的可解释性    328
18.6.1 反卷积网络    331
18.6.2 类别激活映射    334
18.6.3 LIME       339
18.6.4 可视化集成工具Darkon    344
18.7 Auto ML       346

第19章 CV和视觉识别经典模型    348
19.1 CV发展简史       348
19.2 视觉识别概述       353
19.3 R-CNN         359
19.3.1 R-CNN简述    359
19.3.2 R-CNN中的候选区域    360
19.3.3 R-CNN算法处理流程    361
19.4 Fast R-CNN       364
19.5 SPP-Net       365
19.5.1 空间金字塔池化    366
19.5.2 特征图和原图的映射关系    367
19.5.3 基于SPP-Net的目标识别    367
19.6 Faster R-CNN       368
19.6.1 Faster R-CNN简述    368
19.6.2 候选区域网络    370
19.6.3 分类器和边框回归    375
19.7 YOLO          375
19.8 SSD          383
19.8.1 SSD的网络框架    383
19.8.2 SSD的应用推理过程    384
19.8.3 SSD的性能评估和缺点    388
19.9 不基于CNN来实现目标识别   390
19.9.1 相关的OpenCV函数    390
19.9.2 利用OpenCV识别形状物体范例    394

第20章 自然语言处理和CNN    397
20.1 NLP简述          397
20.2 NLP发展历史       399
20.3 自然语言基础       400
20.4 词的表达方式       403
20.5 自然语言模型       405
20.5.1 基于N-Gram的语言模型    406
20.5.2 基于神经网络的语言模型—经典NNLM    409
20.5.3 基于神经网络的语言模型—NNLM的改进者CBOW模型    411
20.5.4 基于神经网络的语言模型—NNLM的改进者Skip-gram模型      414
20.6 word2vec          416
20.6.1 word2vec简介    416
20.6.2 word2vec源码与编译    417
20.6.3 word2vec使用范例    418
20.7 常用语料库       420
20.8 NLP应用：文本分类    424
20.8.1 传统的文本分类方法    424
20.8.2 基于深度学习的文本分类方法    425

第21章 自然语言处理和CNN    430
21.1 应用程序场景识别背景    430
21.2 特征向量          431
21.3 数据采集          432
21.4 算法模型          433
21.5 落地应用          433

第22章 软件自动修复    436
22.1 什么是软件自动修复    436
22.1.1 软件自动修复的定义    436
22.1.2 软件自动修复的价值    437
22.2 软件自动修复基础知识    437
22.2.1 软件自动修复技术分类    437
22.2.2 软件自动修复基础概念    439
22.3 阶段1：缺陷定位    441
22.3.1 基于程序频谱的缺陷定位    443
22.3.2 SFL中测试套件的构造    447
22.3.3 SFL中程序频谱的构造    451
22.4 阶段2：补丁生成    458
22.4.1 基于搜索的补丁生成和自动修复    459
22.4.2 基于模板的补丁生成和自动修复    460
22.5 APR领域经典框架    462
22.5.1 Facebook SapFix    463
22.5.2 Microsoft DeepCoder    465
22.5.3 GenProg    474

第23章 基于强化学习的经典应用—AlphaGO    479
23.1 AlphaGO简述       479
23.2 AlphaGO核心原理    480
23.3 策略网络          481
23.4 估值网络          483
23.5 MCTS          483

机器学习平台篇

第24章 分布式机器学习框架基础知识    488
24.1 分布式机器学习核心理念    488
24.2 GPU硬件设备       491
24.2.1 GPU架构    492
24.2.2 GPU的共享访问    494
24.3 网络标准          498
24.3.1 Ethernet    498
24.3.2 InfiniBand    499
24.4 分布式通信框架    500
24.4.1 MPI       500
24.4.2 P2P和聚合通信    503
24.4.3 NCCL       505
24.4.4 NV-Link    508
24.4.5 RDMA       510
24.5 经典分布式ML框架Caffe-MPI    511

第25章 Tensorflow    514
25.1 Tensorflow安装过程    514
25.2 Tensorflow基础知识    516
25.2.1 Tensorflow核心概念    516
25.2.2 Tensorflow模型/数据的保存和恢复    519
25.2.3 Tensorflow模型fine-tuning    523
25.2.4 Tensorflow模型调试    526
25.2.5 Tensorflow的多语言支持    528
25.2.6 可视化利器TensorBoard    529
25.3 Tensorflow分布式训练    533
25.3.1 Tensorflow的分布式原理    533
25.3.2 单机多GPU下的并行计算    535
25.3.3 多机多GPU下的分布式计算    542
25.4 Tensorflow分布式部署    549
25.4.1 Tensorflow Serving概述    549
25.4.2 基于GPU的Tensorflow Serving    549
25.4.3 Tensorflow Serving的核心概念    552
25.4.4 Tensorflow模型分布式部署实例    553
25.5 Tensorflow范例解析    560
25.6 Tensorflow的“变种”    563
25.6.1 Tensorflow Lite    563
25.6.2 Tensorflow RS    565

第26章 Caffe       568
26.1 Caffe的安装       568
26.1.1 Ubuntu下安装Caffe    568
26.1.2 Windows下安装Caffe    572
26.2 Caffe支持的数据集格式    587
26.2.1 LevelDB   587
26.2.2 LMDB       590
26.2.3 数据库的生成    592
26.3 Caffe中的网络模型构建    594
26.4 Google Protocol Buffer    598
26.5 Caffe2源码结构    600
26.6 Caffe工程范例       601
26.7 Caffe中的Model Zoo    607

第27章 scikit-learn   609
27.1 scikit-learn的安装    610
27.2 scikit-learn中的机器学习算法    610
27.2.1 分类       610
27.2.2 回归       611
27.2.3 聚类       611
27.2.4 降维       612
27.3 scikit-learn中的Model selection    613
27.3.1 网络搜索    613
27.3.2 交叉验证    616
27.3.3 度量标准    616
27.4 scikit-learn中的预处理    619
27.4.1 数据标准化等预处理    619
27.4.2 数据特征提取预处理    621

第28章 主流AI云平台    628
28.1 Microsoft OpenPAI    628
28.2 Google Cloud       631
28.3 Baidu          631
28.3.1 百度AI云服务    632
28.3.2 PaddlePaddle    636
28.4 Alibaba         637
28.4.1 阿里飞天平台    638
28.4.2 MaxCompute平台    639
28.4.3 PAI       640

第29章 图像处理基础    650
29.1 光、色彩和人类视觉系统    650
29.2 图像的颜色模型    653
29.3 图像的基本属性    655
29.3.1 灰度值       655
29.3.2 亮度       656
29.3.3 对比度       657
29.3.4 色相       658
29.3.5 饱和度       658
29.4 图像特征          659
29.4.1 颜色特征    659
29.4.2 纹理特征    660
29.4.3 形状特征    661
29.5 图像的典型特征描述子    661
29.5.1 LBP       661
29.5.2 HOG      677
29.5.3 Haar-like 特征    681
29.5.4 图像的傅里叶变换    686
29.6 图像处理实例(图像质量检测)    690

第30章 程序切片技术    693
30.1 程序切片综述       693
30.2 程序切片基础知识    695
30.2.1 控制流图    695
30.2.2 控制流分析    699
30.2.3 数据流       706
30.3 静态切片技术       715
30.3.1 基本定义    715
30.3.2 静态切片算法    717
30.4 动态切片技术       721
30.4.1 动态切片基本概念    721
30.4.2 动态切片算法概述    723
30.4.3 基于PDG的动态切片算法    723

第31章 业界主流数据集分析    726
31.1 ImageNet简述       726
31.2 ImageNet的构建逻辑    726
31.3 ImageNet数据源的选择与处理    730
31.4 ImageNet的下载    733

参考文献       736


## 神经网络

### 什么是神经网络

### 激活函数

### 卷积神经网络

### RNN循环神经网络

### LSTM （Long Short-Term Memory）

> 一种特殊的递归神经网络

## 备注

> 最后修改时间：2023-02-16 11:17